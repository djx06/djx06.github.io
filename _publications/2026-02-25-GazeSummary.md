---
title: "GazeSummary: Exploring Gaze as an Implicit Prompt for Personalization in text-based LLM Tasks."
collection: publications
permalink: /publication/2026-02-25-gazesummary
# excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2026-02-25
venue: 'hotMobile'
paperurl: 'https://arxiv.org/abs/2601.17676'
citation: 'Jiexin Ding, Yizhuo Zhang, Xinyun Liu, Ke Chen, Yuntao Wang, Shwetak Patel, and Akshay Gadre. 2026. GazeSummary: Exploring Gaze as an Implicit Prompt for Personalization in Text-based LLM Tasks. In The 27th International Workshop on Mobile Computing Systems and Applications (HotMobile 26), February 25â€“26, 2026, Atlanta, GA, USA. ACM, New York, NY, USA, 9 pages. </i>'
---
<!-- This paper is about the number 1. The number 2 is left for future work. -->

Smart glasses are accelerating progress toward more seamless and personalized LLM-based assistance by integrating multimodal inputs. Yet, these inputs rely on obtrusive explicit prompts. The advent of gaze tracking on smart devices offers a unique opportunity to extract implicit user intent for personalization. This paper investigates whether LLMs can interpret user gaze for text-based tasks. We evaluate different gaze representations for personalization and validate their effectiveness in realistic reading tasks. Results show that LLMs can leverage gaze to generate high-quality personalized summaries and support users in downstream tasks, highlighting the feasibility and value of gaze-driven personalization for future mobile and wearable LLM applications.


[Download paper here](https://arxiv.org/abs/2601.17676)

<!-- Recommended citation: Your Name, You. (2009). "Paper Title Number 1." <i>Journal 1</i>. 1(1). -->